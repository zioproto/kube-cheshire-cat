# Pod equivalent to this container:
# docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:0.9.3 --model-id $model --num-shard $num_shard

---
apiVersion: v1
kind: Pod
metadata:
  name: llama-2-70b-chat-hf
  labels:
    run: llama-2-70b-chat-hf
spec:
  containers:
    - name: text-generation-inference
      image: ghcr.io/huggingface/text-generation-inference:0.9.3
      resources:
        limits:
          nvidia.com/gpu: 2
      env:
        - name: RUST_BACKTRACE
          value: "1"
        - name: HUGGING_FACE_HUB_TOKEN
          value: "CHANGE_ME"
        #- name: NCCL_SHM_DISABLE
        #  value: "1"
      #command: ["--model-id", "$model", "--num-shard", "$num_shard"]
      command:
        - "text-generation-launcher"
        - "--model-id"
        - "meta-llama/Llama-2-70b-chat-hf"
        - "--num-shard"
        - "2"
      ports:
        - containerPort: 80
          name: http
      volumeMounts:
        - name: llama270b
          mountPath: /data
        - name: shm
          mountPath: /dev/shm
  volumes:
    - name: llama270b
      persistentVolumeClaim:
        claimName: llama270b
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 1Gi
  nodeSelector:
    agentpool: gpunp
  tolerations:
    - key: sku
      operator: Equal
      value: gpu
      effect: NoSchedule
  restartPolicy: Never
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
    name: llama270b
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: managed-csi-premium
  resources:
    requests:
      storage: 500Gi
---
apiVersion: v1
kind: Service
metadata:
  name: llama-2-70b-chat-hf
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    run: llama-2-70b-chat-hf
  type: ClusterIP

